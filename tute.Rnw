\documentclass{article}

<<set_default_chunk_options,cache=FALSE,echo=FALSE>>=
opts_chunk$set(echo=TRUE,
               warning=TRUE,
               error=TRUE,
               message=TRUE,
               include=TRUE,
               cache=TRUE)
@

\begin{document}

\section{Aims}

So the point of this is multi-faceted:
\begin{itemize}
  \item To provide a tutorial/introduction and access 
  to the use of some of the crappy R code I wrote in 
  the process of analysing peaklist MALDI imaging data 
  during my thesis.
  \item To provide some example code on how (not) to 
  use R, {\tt knitr}, and \LaTeX{}, including 
  referencing using {\tt bibtex}.
\end{itemize}

\section{Disclaimer}

This code is a steaming pile of crap. 
Use it with extreme skeptisism -- as you should use any 
software, and do any analysis, skeptisism is the 
lifeblood of a scientist, embrace it.
When I get some free time I plan on re-writing all the 
plotting functions from scratch to make them nicer.
Which should make this steaming pile of crap slightly
more bearable.

Also, I really ought to have wrapped my code up in a 
package, but I am shit and haven't, so for the time 
being all the relevant functions can be loaded into the \
workspace by calling
<<localFunctions, cache=FALSE, warning=FALSE, message=FALSE>>=
source('localFunctions.R')
@
the fact that this is here may prompt me to getting 
around to cleaning it up and wrapping it up in package
form at some point in the future, but probably not in
the immediate future.


\section{Setup and Reading Peaklists}


I set the current {\tt dataset\_name} to a variable,
<<dataset_name>>=
dataset_name <- "A1"
@

and then call 
<<read_data, dependson="dataset_name", results='hide'>>=
pl_all <- readPeaklists(dataset_name)
@

This reads in peaklist files from 
\begin{verbatim} 
<parent_folder_name>/<dataset_name>/<peaklist_folder_name>
\end{verbatim}
and returns the number of empty spectra after writing 
the three files: 
\begin{verbatim} 
<dataset_name>_comprehensive_peaklist.txt 
<dataset_name>_fExists.txt
<dataset_name>_LXY.txt
\end{verbatim} 
to 
\begin{verbatim} 
./<data_folder>
\end{verbatim} 
{\tt dataset\_name } is required, but the other 
arguments are optional and default to:
\begin{itemize}
  \item {\tt peaklist\_folder\_name  = "peaklists" }
  \item {\tt parent\_folder\_name = "." }
  \item {\tt data\_folder = "./data" }
\end{itemize}


Once created the files written to {\tt data\_folder} 
can be read in easily by calling
<<load_data, dependson="read_data">>=
pl_all  <- load_peaklist(dataset_name)
LXY     <- load_LXY(dataset_name)
fExists <- load_fExists(dataset_name)
@
respectively.
These {\tt load\_*} functions also accept an optional 
{\tt data\_folder} argument if an alternative location
is used to store these files.


\section{Peak Grouping and Peaklist Subsets}

There are three functions included for assigning 
`peakgroup' labels to peaks:

\subsection{Mass Matching}
\label{sec:mm}

Peaks can be matched to known masses by mass-error.
In these data for example, there are some internal 
calibrants of known mass, as described by 
\cite{Gustafsson2012}.
<<cal_info>>=
cal_df = data.frame(m.z = c(1296.685,
                            1570.677,
                            2147.199,
                            2932.588
                    ),
                    name = c('Angiotensin I',
                             '[Glu1]-Fibrinopeptide B',
                             'Dynorphin A',
                             'ACTH fragment (1â€“24)'))
@

The function {\tt mzMatch} extracts peaks 
from the first argument about the $m/z$ values in the 
second argument.
<<mass_matching, dependson=c("load_data","cal_info")>>=
pl_cal <- mzMatch(pl_all,cal_df$m.z)
@
Optional arguments {\tt binMargin} (mass error to be 
allowed) and {\tt use\_ppm} (mass error measured in ppm 
or Da) can also be specified, but otherwise default to:
\begin{itemize}
  \item {\tt binMargin} $=0.3$, and
  \item {\tt use\_ppm} $=$ FALSE.
\end{itemize}
The function {\tt mzMatch} returns the 
subset of the input peaklist with an added column,
{\tt PeakGroup}, specifying the theoretical mass that
peak is matched too.
Not sure how this would react to overlapping mass
windows but it was not intended for that.
  
\subsection{Tolerance Clustering}
\label{sec:tolClus}

When the masses of interest are not known, peakgroups
can be formed via a one-dimensional clustering of the
$m/z$ values of the peaks. Tolerance Clustering is one 
of the simplest ways of doing this, and boils down to 
finding the equivalence classes of the relation defined
on two peaks as `being within some tolerance {\tt tol}
of each other'.
<<tol_clus, dependson="load_data">>=
pl_tol <- groupPeaks(pl_all)
@
The optional tolerance argument {\tt tol} defaults to a 
value of $0.1$Da, and an additional optional argument 
{\tt minGroupSize} can be specified to label any 
equivalence classes with less than that many peaks in 
them zero. By default all peaks will be labeled.
The function {\tt groupPeaks} returns the the input 
peaklist with an added column, {\tt PeakGroup}, 
specifying a peakgroup label.


\subsection{{\tt DBSCAN}}
\label{sec:dbscan}

A more sophisticated clustering approach is to use a 
density based clustering such as {\tt DBSCAN}, or more
precisely its deterministic version {\tt DBSCAN*}, as 
described in \cite{Campello2013}.
<<dbscan_clus, dependson="load_data">>=
pl_dbs <- dbscan_lw(pl_all,pp=FALSE)
@
The function {\tt dbscan\_lw} works similarly to the 
function {\tt groupPeaks}, in that it takes a peaklist 
and returns the same peaklist with an added column, 
{\tt PeakGroup}, containing a peakgroup label.
The function {\tt dbscan\_lw} also takes optional 
arguments {\tt eps} (similar to the {\tt tol} of the 
tolerance clustering, specifying the width of the 
rectangular kernal used), {\tt mnpts} (the minimum 
number of points within a {\tt eps}-neighbourhood
considered significant -- adjusting {\tt mnpts} can fix 
the problem in large datasets of different masses being
combined), {\tt cvar} (specifying the variable in the
input peaklist to be clustered) and {\tt pp} (print 
progress to console logical). These optional arguments 
default to:
\begin{itemize}
  \item {\tt eps} $=0.05$,
  \item {\tt mnpts} $=100$,
  \item {\tt cvar} $=${\tt "m.z"}, and
  \item {\tt pp} $=${\tt TRUE}
\end{itemize}

\subsection{Evaluating Parameter Choices for Peak Grouping}

Both the tolerance clustering of Section~\ref{sec:tolClus}
and the {\tt DBSCAN} approach of Section~\ref{sec:dbscan}
involve the choice of a number of tuning parameters, in 
the former only in grouping tolerance {\tt tol} and in 
the latter both a grouping tolerance {\tt eps} and a 
minimum density {\tt mnpts}. In this section I'll 
demonstrate how to make a heuristic diagnostic plot 
I use as a sanity check that my parameter selection has
been somewhat reasonable. First I summarise the peak 
grouping by calculating a number of statistics for each
peak group:
<<pgs, dependson=c("tol_clus","dbscan_clus")>>=
pgs_tol <- ddply(pl_tol,
                 "PeakGroup",
                 summarise,
                 AWM = weighted.mean(m.z,log1p(intensity)),
                 Range = max(m.z) - min(m.z),
                 nPeaks  = length(m.z),
                 nDupPeaks  = (length(m.z) 
                 - length(unique(Acquisition))))
pgs_dbs <- ddply(subset(pl_dbs, PeakGroup!=0),
                 "PeakGroup",
                 summarise,
                 AWM = weighted.mean(m.z,log1p(intensity)),
                 Range = max(m.z) - min(m.z),
                 nPeaks  = length(m.z),
                 nDupPeaks  = (length(m.z) 
                 - length(unique(Acquisition))))
@
In particular: 
\begin{itemize}
  \item The {\tt AWM}: the abundance weighted mean 
  \emph{m/z} of peaks in the peakgroup -- this is mostly 
  useful for reference when matching particular peakgroups
  of interest to LC-MS/MS identifications, or some such, 
  for example.
 \item The {\tt Range}: the difference between 
  the minimum and maximum \emph{m/z} in the peak group.
  \item The {\tt nPeaks}: the number of peaks in the 
  peak group.
  \item The {\tt nDupPeaks}: the number of duplicate 
  peaks, or the number of occurrances of more than one 
  peak from the same spectrum being included in the 
  same peakgroup
\end{itemize}
Ideally, the {\tt Range} should be small, and the 
{\tt nDupPeaks} should be zero. 
In reality, compromises must be made.
When using the simplistic tolerance clustering 
approach, optimising parameter choice is 
correspondingly simple.
You could for example consider a plot such as 
Figure~\ref{fig:eval_pgs_tol}:
<<eval_pgs_tol, dependson="pgs", fig.cap='Plot for heuristic evaluation of peak grouping efficacy: tolerance clustering.', fig.align='center', out.width='0.6\\linewidth'>>=
p = (ggplot(pgs_tol,aes(x=Range,y=nDupPeaks))
  + geom_point()
  + ggtitle(paste('tol:',toString(0.1))))
print(p)
@
The {\tt nPeaks} is also useful as peakgroups with 
{\tt nPeaks} too small will typically be uninteresting,
and so you could consider the number of peak groups 
with at least some threshold value of {\tt nPeaks}, you 
could apply a similar filter when making a plot such as
Figure~\ref{fig:eval_pgs_tol} in order to get rid of 
some of the junk lying around the origin. 
For peptide data, typically you shouldnt really have 
any peakgroups with a {\tt Range} above $1$ Da, and any
half-decent quality peakpicking should allow for you do 
do peak peaking that keeps the {\tt nDupPeaks} in the 
single digits, generally speaking. So 
Figure~\ref{fig:eval_pgs_tol} shows that this is really
quite terrible, and we should try a smaller value of 
{\tt tol}. Hopefully, reproducing this analysis with a 
smaller {\tt tol} will improve these things. 
Generally, you want to choose a {\tt tol} as large as 
possible so that the {\tt Range} and {\tt nDupPeaks} 
are somewhat reasonable. 
Sometimes this is not possible -- there simply is no 
happy medium, no value of {\tt tol} for which simple 
tolerance clustering produces nice peak groups.
This is usually when using a more sophisticated 
approach, such as {\tt DBSCAN}, is a good idea.
Being a more sophisticated approach, it also involves 
more tuning paramters, more factors to consider.
We can have a look at Figure~\ref{fig:eval_pgs_dbs}, 
analogous to Figure~\ref{fig:eval_pgs_tol}:
<<eval_pgs_dbs, dependson="pgs", fig.cap='Plot for heuristic evaluation of peak grouping efficacy: {\\tt DBSCAN} clustering.', fig.align='center', out.width='0.6\\linewidth'>>=
p = (ggplot(pgs_dbs,aes(x=Range,y=nDupPeaks))
  + geom_point()
  + ggtitle(paste(
    'eps: ',toString(0.05),
    ' mn: ',toString(100),
    ' unassigned: ',
    toString(sum(pl_dbs$PeakGroup==0)),' (',
    toString(round(100*sum(pl_dbs$PeakGroup==0)/nrow(pl_dbs),1)),
    '%)',sep="")))
print(p)
@
The interpretation of Figure~\ref{fig:eval_pgs_dbs} 
itself is similar to that of Figure~\ref{fig:eval_pgs_tol},
the main differences are two-fold: 
\begin{itemize}
  \item There is an extra tuning parameter (density, {\tt mnpts}) 
  which can be thought of as the extra degree of freedom allowing
  us to find nice solutions which cannot be acheived with tolerance
  clustering alone.
  
  \item {\tt DBSCAN} will not allocate some number of peaks to 
  any peakgroup -- {\tt dbscan\_lw()} will assign these peaks a
  {\tt PeakGroup} of $0$. As this is essentially throwing 
  away the information in the peaks, you kinda want to minimise
  how much data you throw away in order to acheive a nice 
  peak grouping, so this adds an additional factor into the 
  equation when trying to tune your tuning parameters.
\end{itemize}
When trying to pick some tuning parameters, I will 
typically produce a bunch of plots like Figure~\ref{fig:eval_pgs_dbs}
with various (reasonable) choices for tuning parameters and 
then try to choose a healthy medium between all the different
factors I'm trying to optimise.
  




\section{Simple Plots}
I also have some functions that make plots specially 
for peaklist data. 
This part is the most hacked up, as I have repeatadly 
modified these functions to perform various different
tasks, while trying to maintain backwards compatability
and its all ended in a giant mess.
These are such a mess I am not even going to bother 
trying to explain them, and instead just reccomend you 
write your own plotting functions, as then you can be 
sure your plotting the thing you want to plot. 
Your also welcome to look at the code under 
{\tt spatialPlot} and {\tt acquisitionPlot} and 
canabilise code to your hearts content.
I provide an example of one use of the function 
{\tt spatialPlot} below when I produce a DIPPS map 
using it, although it can do alot more than this -- I 
plan on coming back and re-writing the plotting 
functions at some point so they make more sense.
But good luck on that ever happening, hahaha.

\section{DIPPS}

Now say you have produced some peakgroups one way or
another, and now you have two regions you want to 
compare using DIPPS. 
For example, here I have annotation of the center of 
cancer tumours stored in an xml `ROI' file.
So I'll read the annotations into R using the 
{\tt XML} package and merge them onto the {\tt LXY} 
variable as a `ROI' column with value `None' for 
spectra not in any annotated region.
<<rois, warning=FALSE>>=
library(XML)
fname <- 'A1_annotation.xml'
doc   = xmlInternalTreeParse(fname)
rois  = xpathSApply(doc,
                   "/ClinProtSpectraImport/Class",
                   xmlGetAttr,"Name")
nSpec = xpathSApply(doc,
                    "/ClinProtSpectraImport/Class",
                    xmlSize)
spec  = xpathSApply(doc,
                    "/ClinProtSpectraImport/Class/Element",
                    xmlGetAttr,"Path")
temp  = data.frame(Peaklist = Peaklist_ID(spec),
                   ROI = rep(rois,nSpec),
                   stringsAsFactors = FALSE)
LXY <- merge(LXY,temp,
             all.x = TRUE)
LXY[is.na(LXY$ROI),'ROI'] = "None"
@

We can take a quick look at these annotations by 
plotting them. Figure~\ref{fig:plot_rois} demonstrates
this, as well as providing a simple (less confusing?)
example of a straightforward way to make spatial plots
without using my gargantuan {\tt spatialPlot} function
(although you could equally make this plot using 
{\tt spatialPlot} if you really wanted to.

<<plot_rois, dependson=c("rois","load_data"), fig.cap='Annotation Regions', fig.align='center', out.width='0.6\\linewidth'>>=
p = (ggplot(LXY,aes(x=X,y=Y,
                    fill=ROI,
                    alpha= 1-(ROI=='None')),
            colour=NA)
  + geom_tile()
  + coord_fixed()
  + guides(alpha = FALSE)     
  + scale_x_reverse(breaks=seq(75,175, 50))
  + scale_y_continuous(breaks=seq(50, 200, 50))
  + ylab("")
  + xlab("")
)
print(p)
@

Now I create a simplified peaklist variable 
{\tt pl\_uni} which intially has only two variables,
{\tt Acquisition} (indexing the originating 
spectrum) and {\tt PeakGroup} (indexing the peakgroup).
I also make sure the rows of {\tt pl\_uni} are unique --
this is important, as having multiple peaks from the 
same peakgroup in the same spectrum will otherwise 
affect your results, although in the senario that this 
occurs more than a couple of times I would suggest 
perhaps revisiting whatever decisions you made at your
peakgrouping step.
I add a third variable, {\tt Group} to the peaklist 
{\tt pl\_uni}, identifying each peak as originating 
from either an annotated region ($2$), or not ($1$).
Now that we have cleaned up {\tt pl\_uni} and ensured 
it has the three neccessary columns, and that they are 
correctly formatted we can plug this right into the 
{\tt DIPPS} function.

<<dipps, dependson="tol_clus">>=
pl_uni = unique(pl_tol[,c("Acquisition","PeakGroup")])
pl_uni = subset(pl_uni,PeakGroup!=0)
temp = match(pl_uni$Acquisition,LXY$Acquisition)
pl_uni$Group = 1+(LXY[temp,]$ROI != "None")

dipsum = DIPPS(pl_uni)
nStar = dippsHeur(pl_uni,dipsum)
@

Note that in this case I used {\tt pl\_tol} to make 
{\tt pl\_uni} which I used in the DIPPS analyses -- this
was the peaklist with the {\tt PeakGroup} column 
created by tolerance clustering. 
I could easily have used {\tt pl\_dbs} or even 
{\tt p\_cal} (if I was only interested in the 
calibrants) instead of {\tt pl\_tol}. 
Another option would be to bin the peaks in a 
data-independant manner using the R {\tt base} function
{\tt cut} -- a potentially useful option when 
interested in prediction, because of its independance 
on the data.

Also note how I generate {\tt nStar}, which is the 
number of variables suggested to be optimal by the 
heuristic. 
One way to visualise these top {\tt nStar} `DIPPS 
features' is in a `DIPPS map', so I might as well 
demonstrate how to do that using the hugely dodgey 
{\tt spatialPlot} function:

<<dipps_map, dependson="dipps", fig.cap='DIPPS Map', fig.align='center', out.width='0.6\\linewidth', message=FALSE>>=
dipsum = dipsum[rev(order(dipsum$d)),]
peakgroups = dipsum[1:nStar,"PeakGroup"]
plp = pl_uni[!is.na(match(pl_uni$PeakGroup,peakgroups)),]
p = spatialPlot(plp,
                fExists,
                plot_var = "count",
                save_plot = FALSE,
                minX_in = min(LXY$X),
                minY_in = min(LXY$X)
)
p = (p + scale_x_reverse(breaks=seq(75,175, 50))
  + scale_y_continuous(breaks=seq(50, 200, 50))
  + ylab("")
  + xlab("")
)
# DIPPS Map
print(p)
@

One could also produce individual intensity plots of 
particular peakgroups, for example for the peakgroup 
with highest DIPPS statistic value: 

<<ion_map, dependson="dipps_map", fig.cap='Intensity Map for peakgroup most highly ranked by DIPPS', fig.align='center', out.width='0.6\\linewidth', message=FALSE>>=
plp = subset(pl_tol,PeakGroup==dipsum[1,"PeakGroup"])
p = spatialPlot(plp,
                fExists,
                save_plot = FALSE,
                minX_in = min(LXY$X),
                minY_in = min(LXY$X)
)
p = (p + scale_x_reverse(breaks=seq(75,175, 50))
  + scale_y_continuous(breaks=seq(50, 200, 50))
  + ylab("")
  + xlab("")
)
# Intensity Map
print(p)
@

and you could for example have a look at the {\tt AWM}
or various other statistics for these peakgroups if you 
wanted. For example:

<<awm, dependson=c("dipps_map","pgs_dbs")>>=
pgs_tol = merge(pgs_tol,dipsum)
pgs_tol = pgs_tol[rev(order(pgs_tol$d)),]
print(head(pgs_tol), digits=3)
print(pgs_tol[57:61,], digits=3)
@

In addition to calculating the {\tt AWM} here I have 
also calculated the $m/z$ range over which each 
peakgroup spans, and the number of duplicated peaks 
(a number above zero indicated there are individual 
spectra with more than one peak in the indicated 
peakgroup).
Notice that the most highly ranked variables by DIPPS 
seem fine, but that there are some (AWM = $1155$ and 
AWM = $1776$) for which it seems the peakgrouping has 
failed pretty badly, producing peakgroups that span 
$4.16$Da and $2.789$Da respectively.
Note that you can look at such things without having 
done the DIPPS step, and it is worth doing so as a 
quality-control/sanity-check step.
You could for example try using the dbscan peakgroups
instead -- I'll leave that as an exercise.




\pagebreak
\bibliographystyle{plain}
\bibliography{references}
\section*{Session Info and pdflatex Version}

<<sessioninfo, echo=TRUE, results='markup', cache=FALSE, size='footnotesize'>>=
sessionInfo()
@

<<pdflatex_version, echo=TRUE, message=TRUE, warning=TRUE, cache=FALSE, size='footnotesize'>>=
system('pdflatex --version',intern=TRUE)
@

\end{document}